7CS039 Statistics for Data Science
Week 2 Probability Distributions
1 Discrete Random Variables
The sample space of an experiment is the “set of all possible outcomes” of the experiment.
We saw that the elements of some sample spaces may take on any value in a given range,
e.g. the life of a lightbulb; other sample spaces consist of specific, discrete values. e.g.
roll a dice,
Sample Space = {1,2,3,4,5,6}
The rolling of a dice can be thought of as a mathematical function. Such a function
is known as a discrete random variable.
1.1 Definition of Discrete random Variable
Definition: A discrete random variable is a function that associates a unique numerical
value with every outcome of a discrete experiment.
Notethat(technically)iftheelementsofthesamplespacearenotthemselvesnumbers,
(for instance, if were to select a colour of the rainbow at random the samples space is
{Red, Orange, Yellow, Green, Blue, Indigo, Violet}
then to set up the random variable we should assign a number to each of the colours (of
course these will usually be 1,2,3,4,5,6,7, but they don’t have to be).
1.2 Examples of Discrete random Variables
Example
Is a train late or not–part one
If we view this as a simple yes or no and let no map to 0 and yes map to 1 then
the rv (random variable) is simply
Sample Space no yes
X 0 1
1
Example
Is a train late or not–part two
It might be decided that the above yes/no approach to a train being late is too
simple, we might wish to categorise lateness as:
On Time, slightly late, moderately late, very late, extremely late
A possible random variable is:
Sample Space On Time slightly late moderately late very late extremely late
X 0 1 2 3 4
Again, we could use any five numbers, in any order, and this would still technically
be valid, but clearly some combinations make little sense.
Example
Tossing a Coin Once
For example:
Sample Space head tail
X 0 1
i.e. X(head) = 0, X(tail) = 1
Example
Tossing a Coin Twice
Here we need to specify whether the order of the outcomes matters, i.e. is TH (a
tail followed by a head) the same as HT (a head followed by a tail)?
If order does not matter HT and TH would both map to the same value and a
possible RV would be:
Sample Space HH TT HT TH
X 0 1 2 2
i.e. X(HH) = 0, X(TT) = 1, X(HT) = X(TH) = 2
but if the order does matter then HT and TH must map to different values and
thus a possible RV is:
Sample Space HH TT HT TH
X 0 1 2 3
i.e. X(HH) = 0, X(TT) = 1, X(HT) = 2, X(TH) = 3
2
Example
The Number of full days until the end of the universe
Here we have an infinite, but discrete, sample space. The sample space is the set
of all non-negative whole numbers. If the universe is going to end in an hour then
their are no full days left, if it will end in a year then there are 365 full days left,
and given that it is possible that the universe will go on forever the sample space
is infinite. Thus a possible random variable here is simply X(t) = t, where X is
the number of full days until the universe ceases.
1.3 Probability Mass Function
A random variable is basically an assignment of numerical values to a sample space, it
says nothing about how probable it is that a given event will occur. If we associate with
each member of a discrete sample space the probability of occurrence of that member we
have a probability mass function (pmf) also known as a Discrete Probability Distribution.
For instance the probability mass function for tossing a fair coin once is:
X Head Tail
P(X) 0.5 0.5
Whereas the pmf for tossing a coin that is weighted so that it will only land “heads” one
time in a hundred is:
X Head Tail
P(X) 0.01 0.99
Similarly the pmf for tossing a fair coin twice, where the order of occurrence is unimpor-
tant is:
X HH TT HT
P(X) 0.25 0.25 0.5
where HT stands for either a head followed by a tail or vice-versa.
What if the sample space is infinite, as in the “end of the universe” example? We
can still have valid probability mass functions, provided the assignment of probabilities
follow the basic rules, i.e for any element s of the sample space:
• 0 ≤ P(s) ≤ 1
(cid:88)
• P(s) = 1
alls
For instance, if we thought that there was a 0.5 probability that the world would
end today, a 0.25 probability that it will end tomorrow, a 0.125 probability that it will
end the next day etc. then P(X = k) = (cid:0) 1 (cid:1)k+1. We can illustrate this probability mass
2
function by the histogram of Figure 1
3
Figure 1: PMF of “end of universe” example
1.4 The expected value and variance of a discrete ran-
dom variable
Let X be a discrete random variable, the expected value of X is:
(cid:88)
E(X) = µ = xP(x)
As with numeric data, the variance of a discrete random variable is the mean of the
squared differences between the vales of the rv and the mean of the rv, i.e.:
(cid:2) (cid:3) (cid:88)
Var(X) = E (X −µ)2 = (X −µ)2P(x)
The expected value of a discrete random variable is also referred to as the expectation
or the mean of X. Similarly, the standard deviation of the discrete random variable is
the square root of the variance.
4
Example
Suppose you buy a packet of six USB sticks. Let X = the number of the six sticks
that are faulty. From experience (e.g. data from the manufacturer) it is known
that the pmf of X is as follows.
x 0 1 2 3 4 5 6
P(X = x) 0.8 0.05 0.03 0.01 0.01 0 0.1
What is the expected number of faulty sticks in a packet? What is the variance?
Answer:
E(X) = µ = (0)(0.8)+(1)(0.05)+(2)(0.03)+(3)(0.01)+(4)(0.01)
+ (5)(0)+(6)(0.1)
= 0.78
(This means that on average a packet of six sticks will contain 0.78 faulty sticks,
i.e. 100 packets would on average contain 78 faulty sticks.)
Var(x)
(cid:88)
= [x−µ]2p(x)
= (0−0.78)2(0.8)+(1−0.78)2(0.05)+(2−0.78)2(0.03)+(3−0.78)2(0.01)
+(4−0.78)2(0.01)+(5−0.78)2(0)+(6−0.782)(0.1)
= 3.4116
Next we look at two special types of discrete probability distributions, the Binomial
and the Poisson distribution.
2 The Binomial Distribution
The Binomial Distribution is the discrete probability distribution of the number of suc-
cesses in a sequence of n independent experiments each with its own outcome: success
(withprobabilityp)orfailure(withprobability1−p). Asinglesuccess/failureexperiment
is also called a Bernoulli trial.
5
Examples
• Toss a coin. Let E be the event that we obtain a head. E either occurs
(success) with probability p , or it does not (failure) with probability 1−p.
• Roll a dice. Let E be the event that we roll a six. E either occurs (success)
with probability p , or it does not (failure) with probability 1−p.
• Let E be the event that somebody dies of a stroke. E either occurs (success)
with probability p , or it does not (failure) with probability 1−p.
Usually we denote the probability that the event we are interested in occurs by the
letter p. Hence the probability that the event we are interested in does not occur is 1−p.
Sometimes we write q for 1−p.
Examples
• If we toss a coin, and are interested in obtaining a head, p = 1, and hence
2
q = 1−p = 1.
2
• If we roll a dice, and are interested in the event that we roll a “six”. p = 1
6
and hence q = 1−p = 5.
6
It can be shown that if the probability of an event occurring is p, then the probability
that the event occurs exactly r times in n trials is:
(cid:18) (cid:19)
n
P(X = x) = px(1−p)n−x
x
6
Example
Suppose that one in five people die of a cancer related illness. If a random sample
of 12 recently deceased people was taken. What is the probability that:
(i) Exactly 2 of the deaths were due to cancer,
(ii) At most 2 of the deaths were due to cancer,
(iii) At least 2 of the deaths were due to cancer?
(i) Here p = 1 = 0.2, ⇒ q = 0.8, n = 12.
5
Let X = The number of the twelve people that die of cancer.
(cid:18) (cid:19)
12
P(X = 2) = (0.2)2(0.8)10 = 0.283
2
(ii) P(At most 2) = P(X ≤ 2) = P(X = 0) + P(X = 1) + P(X = 2) =
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
12 12 12
(0.2)0(0.8)12 + (0.2)1(0.8)11 + (0.2)2(0.8)10 = 0.069 +
0 1 2
0.206+0.283 = 0.558
Example ctd.
(iii) P(At least 2) = P(X ≥ 2). We could calculate:
P(X = 2)+P(X = 3)+······+P(X = 12)
butthiswouldbeextremelytimeconsuming. Itiseasiertoproceedasfollows.
P(X < 2)+P(X ≥ 2) = 1, ⇒ P(X ≥ 2) = 1−P(X < 2)
= 1 − [P(X = 0) + P(X = 1)] = 1 −
(cid:20)(cid:18) (cid:19) (cid:18) (cid:19) (cid:21)
12 12
(0.2)0(0.8)12 + (0.2)1(0.8)11
0 1
= 0.725
7
Example
It is known that 0.16 of women have rhesus-negative blood, what is the probability
that if twenty of them are chosen at random:
(i) Exactly three of them are rhesus-negative?
(ii) Fewer than four of them are rhesus negative?
(iii) More than three of them are rhesus-negative?
(iv) The number of them that are rhesus-negative is between two and four inclu-
sive?
Solution:
Let X be the number of the women who are rhesus-negative. Here n = 20, p = 0.16
(cid:18) (cid:19)
20
(i) P(X = 3) = × 0.163 × (1 − 0.16)(20−3) = 1,140 × 0.004096 ×
3
0.05161166 = 0.241
(ii) P(X ≤ 3) = P(X = 0)+P(X = 1)+P(X = 2)+P(X = 3) = 0.599
(iii) P(X ≤ 3)+P(X > 3) = 1 ⇒ P(X > 3) = 1−P(X ≤ 3) = 1−0.599 = 0.401
(iv)
P(2 ≤ X ≤ 4) = P(X = 2)+P(X = 3)+P(X = 4)
= 0.211+0.241+0.195
= 0.647
2.1 The Expected value of a binomial random variable
In the previous example, what would you “expect” the number of rhesus-negative women
to be? If there are 20 women, and the probability that any one of them is rhesus negative
is 0.16 then it would seem reasonable to “expect” 20×0.16 = 3.2 to be rhesus-negative;
and for any binomial random variable, X it seems reasonable to “expect” np of them
to possess the property of interest. From the definition of the expectation of a random
variable it can be shown that the expectation of a binomial random variable X is indeed:
(cid:18) (cid:19)
(cid:88) (cid:88) n
E(X) = xP(x) = x px(1−p)n−x = np
x
8
Similarly it may be shown that the variance of a binomial random variable is
np(1−p)
3 The Poisson Distribution
The Poisson distribution typically applies to the frequency with which events occur over
a given period of time, or over an area. For example if blood is smeared over a slide and
the number of cells in each millimetre squared is counted, then these numbers typically
follow a Poisson distribution. Similarly the amount of times somebody’s mobile phone
will ring in an hour typically is Poisson distributed.
If X is Poisson distributed with a mean occurrence rate of λ then:
e−λλr
P(X = r) =
r!
Where, λ is the Greek (lower case) letter “lambda”, ‘e’ is an irrational number, ≈ 2.71828.
3.1 Mean & Variance
It can also be shown that if X is a Poisson random variable then
E(X) = λ and Var(X) = λ
9
Example
The number of phone calls per hour that Uzma receives is Poisson distributed with
a mean of one per two hours. What is the probability that she will receive:
(i) Exactly one phone call in the next hour,
(ii) At least two phone calls in the next hour,
(iii) At least two phone calls between 6pm and 10pm,
(iv) at least one phone call in the next minute.
Solution
Here λ = 0.5 per hour.
(i) P(X = 1) = e−0.50.51 = 0.3033
1!
(ii) P(X ≥ 2) = 1−[P(X = 0)+P(X = 1)] = 0.0902
(iii) At least two phone calls between 6pm and 10pm = at least two phone calls
in 4 hours. As λ = 0.5 per hour λ = 2 per 4 hours.
P(X ≥ 2, in 4 hours) = 1−[P(x = 0)+P(x = 1)]
(cid:20) e−220 e−221(cid:21)
= 1− + = 0.5940
0! 1!
(iv) λ = 0.5 per hour ⇒ λ = 0.5 per minute = 0.0083. Hence P(X ≥ 1) =
60
e−0.00830.00830
1−P(X = 0) = 1− = 0.0083.
0!
4 Continuous Random Variables
• Acontinuous random variableisonethatisabletotakeanyvaluewithinagiven
range, including non-integer values. (i.e. its range is continuous). Measurements
such as length of time, mass, electrical current are treated as continuous random
variables.
• Given that a continuous random variable takes an infinite amount of values, even
over a very short interval, it does not make sense to talk about the probability
that a continuous random variable takes a given value. For instance, if we say, for
10
instance, that the probability that a randomly selected male is 180 cms tall is 0.09,
in reality we mean that the probability that he is between, say, 179.5 and 180.5 cms
tall is 0.09. For this reason we talk about the probability density function (pdf) of
a continuous random variable.
• The pdf of a continuous random variable can be thought of as a continuous version
of the relative-frequency histograms we have seen for discrete random variables.
If the pdf is represented as f(x), then f(x) is a valid pdf if and only if:
• f(x) ≥ 0 ∀x
(cid:90) ∞
• f(x)dx = 1
−∞
Note that:
(cid:90) b
P(a ≤ X ≤ b) = f(x)dx
a
You can think of this graphically:
Figure 2: Probability density Function of random variable representing male heights
Note that area under any pdf curve equals one, this reflects the fact that the random
variable has to take some value.
4.1 Skewness
IfarandomvariableX isdistributedsuchthatP(X = E(X)−k) = P(X = E(X)+k),for
any value of k, then we say that the distribution is symmetric, the histogram and boxplot
11
of Figure 3 illustrate such a distribution. Note that the two tails of the distribution are
“mirror images” of each other. If however the tail to the right (as we look at it) of the
histogram, (or alternatively the length of the upper whisker of the boxplot) is longer
than that of the left-hand tail, then we say the distribution is skewed to the right. Figure
4 illustrates such a distribution. This means that values considerably greater than the
mean are more likely to occur than values considerably less than the mean. The reverse
is true of random variables that are skewed to the left. Whilst here we have introduced
the concept of skewness in relation to continuous random variables, the concept is also
applicable (in exactly the same way) to discrete random variables.
Figure 3: Example Symmetric Distribution
12
Figure 4: Example of Right-Skewed Distribution
Figure 5: Example of Left-Skewed Distribution
4.2 Expected Value and Variance of a Continuous Ran-
dom Variable
Analogous to the discrete case, the expected value or mean of a continuous variable X
with pdf f(x) is
(cid:90) ∞
xf(x)dx
−∞
and the variance is:
(cid:90) ∞
(x−µ)2f(x)dx
−∞
the standard deviation being the square root of the variance.
5 The Normal Distribution
Suppose we were to obtain data on the heights of 1,000,000 women, (thus this data is
continuous), and then illustrated the data with a histogram. Note that the mean and
standard deviation of female heights are 163 and 6.5 centimetres respectively. The result
would be extremely similar to the histogram of figure 6.
13
Figure 6: Distribution of female heights
From our knowledge of heights, the distribution illustrated by the histogram should
not surprise us: from the histogram we see that 0.06141200 of females are between 162.5
and 163.5 centimetres tall, and that this interval contains the greatest proportion of
female heights. Either side of this heights fall off symmetrically, as many women are
between 161.5 and 162.5 centimetres tall as are between 163.5 and 164.5 centimetres tall,
as many women are between 160.5 and 161.5 centimetres tall as are between 164.5 and
165.5 centimetres tall, etc. Note also that there are extremely few women with heights
greater than 182.5 centimetres, (i.e. three standard deviations greater than the mean),
or less than 143.5, (i.e. three standard deviations less than the mean).
The above female heights data is an example of normally distributed data, to be more
specific it is normally distributed with mean 163 and standard deviation 6.5. Strictly
speaking, we should say that the data is drawn from a normal distribution, as to have
perfectly normally distributed data, we would have to have an infinite amount. As here
we have one million data, this really is a technicality, but with small amounts of data
it is often not straightforward to determine whether the data is drawn from a normal
distribution or not. The solid line that is superimposed upon the histogram represents
the normal distribution curve and represents how the data would look if it were perfectly
normally distributed. All normally distributed data has the following properties:
• It is bell shaped.
• It is unimodal i.e. it peaks at a single value.
• The mean is located at the centre of the distribution.
• It is symmetrical, with mean=median.
• It approaches the horizontal axis on either side of the mean toward plus and minus
infinity.
14
Figure 7 illustrates the normal distribution curves of data that has a mean of µ = 40
and standard deviations of σ = 2 (dotted line), σ = 5 (dashed line) and σ = 10 (solid
line) respectively.
Figure 7: Normal Distributions, µ = 40
Note that the three distribution curves and all have the properties listed above, and
are centred at the mean of µ = 40, but that as the standard deviation increases, the
15
density curve becomes flatter. This makes sense, the greater the standard deviation, the
more spread out the data, and the hence the more spread out the curve. Note that in all
cases there is little data to left of three standard deviations less than the mean, or to the
right of three standard deviations greater than the mean. Figure 8 reproduces the curves
for the N(40,2), N(40,5) and N(40,15) distributions, note that now these three diagrams
are not all drawn to the same scale, the computer package has resized the diagrams to fit
the box, look at the proportion of each of the three curves that is under the curve to the
left of one standard deviation less than the mean, i.e. to the left of 38 for the N(40,2)
data, to the left of 35 for the N(40,5) curve and to the left of 25 for the N(40,15) curve.
Figure 8:
We note two things:
• In each case approximately 15% (15.87% to be exact) of the area under the normal
curve lies to the left of one standard deviation less than the mean
• If we ignore the numbers on the axes, the three diagrams are the same
This illustrates a very important property of normal distributions: in terms of “stan-
dard deviations from the mean” they are all the same. The standard normal distribution
refers to the normal distribution with µ = 0 and σ = 1.
16
Figure 9: The Standard Normal Distribution
17
