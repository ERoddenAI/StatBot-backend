7CS039 Statistics for Data Science
Week 6 Statistical Classification
1 Introduction
Classification is a common problem encountered as a data scientists. The central aim
of a classifier is to predict the category an object belongs to based on the features or
properties that object has. This can be as simple as classifying an email as spam or not
or as classifying mushroom as poisonous or not (we’ll study this example later).
Classification
Classification is concerned with predicting the target class for an object based
on the features of that object. The possible classes are already known and the
algorithm needs to identify which class the object belongs to.
1.1 Terminology
• Classifier: A classifier is an algorithm that classifies the input data into output
categories.
• Classification model: A classification model is a model that uses a classifier to
classify data objects into various categories.
• Feature: A feature is a measurable property of a data object.
Classification is a form of supervised learning where the response variable is categori-
cal, as opposed to numeric. Our goal is to find a rule, algorithm, or function which takes
as input a feature vector, and outputs a category which is the true category as often as
possible. Our first classifier will use the ISLR data set.
> install.packages("ISLR")
> library(ISLR)
> library(tibble)
1
The Default dataset is a simulated data set containing information on ten thousand
customers. The aim here is to predict which customers will default on their credit card
debt. The dataset has the following format
• default A factor with levels No and Yes indicating whether the customer defaulted
on their debt
• student A factor with levels No and Yes indicating whether the customer is a
student
• balance The average balance that the customer has remaining on their credit card
after making their monthly payment
• income Income of customer
Note that the variables default and student are factor variables. The first thing we do
is split our data into a training set and a test set. We’ll use 50% for each.
> set.seed(20)
> train_index = sample(nrow(Default), 5000)
> train_default = Default[train_index, ]
> test_default = Default[-train_index, ]
2
Now will do some quick visualizations to help us suggest some classification rules. To do
this we will use the caret package.
> install.packages("caret")
> library(caret)
Now we’ll use a density plot to explore properties of the numeric predictors.
featurePlot(x = train_default[, c("balance", "income")],
y = train_default$default,
plot = "density",
scales = list(x = list(relation = "free"),
y = list(relation = "free")),
adjust = 1.5,
pch = "|",
layout = c(2, 1),
auto.key = list(columns = 2))
In this function we have
• x is a data frame containing only numeric predictors.
• y is the response variable. It needs to be a factor variable.
• plot specifies the type of plot, here it’s density.
• scales defines the scale of the axes for each plot. By default, the axis of each plot
would be the same, which often is not useful, so the arguments here, a different axis
for each plot, will almost always be used.
• adjust specifies the amount of smoothing used for the density estimate. (you can
play around with this)
• pch specifies the plot character used for the bottom of the plot.
• layout places the individual plots into rows and columns. It is given as column,
row.
• auto.key defines the key at the top of the plot. The number of columns should be
the number of categories.
3
Theincomevariablebyitselfisnotparticularlyusefulpredictorofwhetherornotsomeone
will default. However, there seems to be a big difference in default status at a balance of
about 1400. We will use this information shortly. Now let’s explore the relationship with
the variable student as the response.
featurePlot(x = train_default[, c("balance", "income")],
y = train_default$student,
plot = "density",
scales = list(x = list(relation = "free"),
y = list(relation = "free")),
adjust = 1.5,
pch = "|",
layout = c(2, 1),
auto.key = list(columns = 2))
4
We can see that students have a much lower income (perhaps not surprising). Now let’s
plot pairs of variables.
featurePlot(x = train_default[, c("student", "balance", "income")],
y = train_default$default,
plot = "pairs",
auto.key = list(columns = 2))
5
This plot also suggests that using balance to create a classifier will be more useful than
income.
2 Our First Classifier
Wealreadynotedthatwhetherornotsomeonedefaultsseemstochangewhenthebalance
variable is around 1400. Let’s (naively) design our classifier as follows
(cid:40)
Yes if balance > 1400
Classify(balance) =
No if balance ≤ 1400
This means that we will predict an individual is a defaulter if their balance is above 1400
and predict that they are not otherwise. Let’s see how our classifier performs. First we
write a simple function to do the classification for us according to the rule above.
> MyFirstClassifier = function(x, boundary, above = 1, below = 0) {
ifelse(x > boundary, above, below)
}
Then we put this to use on the training and test sets.
> training_predictor = MyFirstClassifier(x = train_default$balance,
boundary = 1400, above = "Yes", below = "No")
6
> test_predictor = MyFirstClassifier(x = train_default$balance,
boundary = 1400, above = "Yes", below = "No")
So there we have it. A simple crude classifier which will take an input vector and
predict whether or not the person will default.
3 Is it any good though?
One way to measure how good our classifier is, is to make a table of the predictions and
the true default numbers.
> (trainingset_table = table(predicted = train_pred, actual = train_default$default))
> (testset_table = table(predicted = test_predictor, actual = test_default$default))
These tables are usually referred to as a confusion matrix. The entries in the table
represent (and be careful about this)
7
True no False no
False yes True yes
meaning that the number in the top left is the number or cases correctly identified as no,
the number in the bottom right is the number of classes correctly classified as yes. The
number in the bottom left is the number of cases classified incorrectly as defaulters and
the number in the top right is the number of cases classified incorrectly as not having
defaulted. In statistics we think if these as type I and type II errors. The caret package
includes a handy function for confusion matrices.
> training_con_mat = confusionMatrix(trainingset_table, positive = "Yes")
> training_con_mat
> testset_con_mat = confusionMatrix(testset_table, positive = "Yes")
> testset_con_mat
8
The information included in the output tells us how good our classifier is. There is a
lot of information here but the most important parts are
• Accuracy - the proportion of predicted classes which are correct.
• Sensitivity - the proportion of true positives classified as positive (the true positive
rate if you like)
• Specificity - the proportion of true negatives classified as negative (the true negative
rate if you like)
That’senoughforthissession. Intheworkshopwe’llexploreclassificationinmoredetails.
9